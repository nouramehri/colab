{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOoOhnz4ZIfyfTu5zdYSG5P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nouramehri/colab/blob/main/python2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lab2\n",
        "ex1\n"
      ],
      "metadata": {
        "id": "nxhXAsAVd6-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!myenv/bin/pip install nom_du_package\n",
        "!myenv/bin/python script.py\n"
      ],
      "metadata": {
        "id": "QTPEPgheeH0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex2"
      ],
      "metadata": {
        "id": "VbuSDSe1ee-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fastapi sqlalchemy psycopg2-binary"
      ],
      "metadata": {
        "id": "7Y0uoLC7etmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ex3"
      ],
      "metadata": {
        "id": "E91OZ2fSex2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException, Depends\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Annotated\n",
        "\n",
        "app = FastAPI()"
      ],
      "metadata": {
        "id": "8EcpzHC0e2Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ex4"
      ],
      "metadata": {
        "id": "UYU_TF42giIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class choiceBase(BaseModel):\n",
        "  choice_text: str\n",
        "  is_correct: bool\n",
        "class QuestionBase(BaseModel):\n",
        "  question_text: str\n",
        "  choices: list[choiceBase]"
      ],
      "metadata": {
        "id": "M8x4sY8cgjc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Créer une connexion PostgreSQL\n",
        "ex5"
      ],
      "metadata": {
        "id": "JxGWx2NChR6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "from sqlalchemy.ext.declarative import declarative_base\n",
        "URL_DATABASE='postgresql://USERNAME:PASSWD@localhost:5432/quizApp'\n",
        "engine = create_engine(URL_DATABASE)\n",
        "Sessionlocal = sessionmaker(autocommit=False , autoflush=False , bind=engine)\n",
        "base = declarative_base()\n",
        "Sessionlocal = sessionmaker(autocommit=False , autoflush=False , bind=engine)\n",
        "base = declarative_base()"
      ],
      "metadata": {
        "id": "r9b-ghAShdsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Créer des tables PostgreSQL\n",
        "## ex05"
      ],
      "metadata": {
        "id": "QVyhZaBwjNF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# database.py\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "from sqlalchemy.ext.declarative import declarative_base\n",
        "\n",
        "# URL string for the Postgres database\n",
        "# In this example, the database name is 'quizApp'\n",
        "URL_DATABASE = 'postgresql://USERNAME:PASSWD@localhost:5432/quizApp'\n",
        "\n",
        "engine = create_engine(URL_DATABASE)\n",
        "\n",
        "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
        "\n",
        "Base = declarative_base()"
      ],
      "metadata": {
        "id": "S4rYbn1ejT_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Créer des tables PostgreSQL\n",
        "## ex1"
      ],
      "metadata": {
        "id": "trnXPWCZjkvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine, Column, Integer, String, Boolean, ForeignKey\n",
        "from sqlalchemy.orm import declarative_base, sessionmaker\n",
        "\n",
        "# Déclaration de la base\n",
        "Base = declarative_base()\n",
        "\n",
        "# Modèle Questions\n",
        "class Questions(Base):\n",
        "    __tablename__ = 'questions'\n",
        "    id = Column(Integer, primary_key=True, index=True)\n",
        "    question_text = Column(String, index=True)\n",
        "\n",
        "# Modèle Choices\n",
        "class Choices(Base):\n",
        "    __tablename__ = 'choices'\n",
        "    id = Column(Integer, primary_key=True, index=True)\n",
        "    choice_text = Column(String, index=True)\n",
        "    is_correct = Column(Boolean, default=False)\n",
        "    question_id = Column(Integer, ForeignKey(\"questions.id\"))\n",
        "\n",
        "# Création de la base SQLite en mémoire (ou fichier si tu veux)\n",
        "engine = create_engine(\"sqlite:///quiz.db\", echo=True)\n",
        "Base.metadata.create_all(bind=engine)\n",
        "\n",
        "# Création de session\n",
        "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
        "session = SessionLocal()\n",
        "\n",
        "# Exemple d'ajout de question avec choix\n",
        "question = Questions(question_text=\"Quelle est la capitale de la France ?\")\n",
        "session.add(question)\n",
        "session.commit()\n",
        "\n",
        "choice1 = Choices(choice_text=\"Paris\", is_correct=True, question_id=question.id)\n",
        "choice2 = Choices(choice_text=\"Lyon\", is_correct=False, question_id=question.id)\n",
        "session.add_all([choice1, choice2])\n",
        "session.commit()\n",
        "\n",
        "# Affichage des questions/choix\n",
        "for q in session.query(Questions).all():\n",
        "    print(f\"Question: {q.question_text}\")\n",
        "    for c in session.query(Choices).filter_by(question_id=q.id):\n",
        "        print(f\"  - {c.choice_text} (Correct: {c.is_correct})\")\n",
        "\n",
        "session.close()\n"
      ],
      "metadata": {
        "id": "SaNn3mcIjl_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ex2"
      ],
      "metadata": {
        "id": "ZiEbiN_xkUzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Installation des dépendances nécessaires\n",
        "!pip install sqlalchemy matplotlib-venn\n",
        "!apt-get -qq install -y libfluidsynth1\n",
        "\n",
        "\n",
        "#  Importation des modules et création des tables\n",
        "import models\n",
        "from database import engine, SessionLocal\n",
        "from sqlalchemy.orm import Session\n",
        "\n"
      ],
      "metadata": {
        "id": "D_sK9RbLkctc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ex3"
      ],
      "metadata": {
        "id": "zOfGNDWfl3vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create all the tables and columns in PostgreSQL\n",
        "models.Base.metadata.create_all(bind=engine)"
      ],
      "metadata": {
        "id": "a5o-LZ7Vl6nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connectez FastAPI, PostgreSQL avec SQLAlchemy\n",
        "## ex1"
      ],
      "metadata": {
        "id": "UkB7Ghxwmgab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_db():\n",
        "    db = SessionLocal()\n",
        "    try:\n",
        "        yield db\n",
        "    finally:\n",
        "        db.close()"
      ],
      "metadata": {
        "id": "INGgkL6Pmj0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ex2"
      ],
      "metadata": {
        "id": "6P4j4RTGmmQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This will be used later for a dependency injection\n",
        "db_dependency = Annotated[Session, Depends(get_db)]"
      ],
      "metadata": {
        "id": "5aeY42C2mnxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FastAPI Ajouter un point de terminaison de question\n",
        "# ex1"
      ],
      "metadata": {
        "id": "zn3D3vXsmqHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We passing in a data validation to validate the body of the API request\n",
        "# and passing the database connection\n",
        "@app.post('/questions/')\n",
        "def create_questions(question: QuestionBase, db: db_dependency):\n",
        "    # Here we use SQL Alchemy to write an ORM statement that will link with the database\n",
        "    db_question = models.Questions(question_text=question.question_text)\n",
        "    db.add(db_question)\n",
        "    db.commit()\n",
        "    db.refesh(db_question)\n",
        "    for choice in question.choices:\n",
        "        db_choice = models.Choices(choice_text=choice.choice_text, is_correct=choice.is_correct, question_id=db_question.id)\n",
        "        db.add(db_choice)\n",
        "    db.commit()"
      ],
      "metadata": {
        "id": "HLU5k-0rmrPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ex2"
      ],
      "metadata": {
        "id": "4XHddxZym3p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uvicorn main:app --reload"
      ],
      "metadata": {
        "id": "bs5go0cEm6o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ex3"
      ],
      "metadata": {
        "id": "ObjDPA08m93N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"question_text\": \"What is the best Python Framework\",\n",
        "  \"choices\": [\n",
        "    {\n",
        "      \"choice_text\": \"FastAPI\",\n",
        "      \"is_correct\": True\n",
        "    },\n",
        "    {\n",
        "      \"choice_text\": \"Flask\",\n",
        "      \"is_correct\": False\n",
        "    },\n",
        "    {\n",
        "      \"choice_text\": \"Django\",\n",
        "      \"is_correct\": False\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "O2PSDVkLm_AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autres points de terminaison FastAPI\n",
        "## ex1"
      ],
      "metadata": {
        "id": "WzdGkG9XnKCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@app.get('/questions/{question_id}')\n",
        "def read_questions(question_id: int, db: db_dependency):\n",
        "    result = db.query(models.Questions).filter(models.Questions.id == question_id).first()\n",
        "    if not result:\n",
        "        raise HTTPException(status_code=404, detail='Question is not found!')\n",
        "    return result"
      ],
      "metadata": {
        "id": "8BSQkeKynNZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex2"
      ],
      "metadata": {
        "id": "N7YBls0fnS6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@app.get('/choices/{question_id}')\n",
        "def read_choices(question_id: int, db: db_dependency):\n",
        "    result = db.query(models.Choices).filter(models.Choices.question_id == question_id).all()\n",
        "    if not result:\n",
        "        raise HTTPException(status_code=404, detail='Choices is not found!')\n",
        "    return result"
      ],
      "metadata": {
        "id": "68ilrKdqnWAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tutoriel Python sur le scraping Web"
      ],
      "metadata": {
        "id": "srenkV9OnZ2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objectifs du laboratoire\n",
        "ex1"
      ],
      "metadata": {
        "id": "8Dm9cMRRncPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Faire une requête HTTP\n",
        "## ex1"
      ],
      "metadata": {
        "id": "hCPh1LJGn46S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print('Hello world!')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "apAPIVDan7tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ex2"
      ],
      "metadata": {
        "id": "fH3F2G7HoJnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m venv .venv\n",
        "!source .venv/bin/activate   # Pour activer l'environnement virtuel"
      ],
      "metadata": {
        "id": "QSJ4kT8ooLOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ex3"
      ],
      "metadata": {
        "id": "MEo0KpwcoWOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!.\\.venv\\Scripts\\activate"
      ],
      "metadata": {
        "id": "LTGRiJwfoXOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ex4"
      ],
      "metadata": {
        "id": "xXW2ujfipjDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scraper.py"
      ],
      "metadata": {
        "id": "YfMeCpImpk62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ex5"
      ],
      "metadata": {
        "id": "0-NehYd8prUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  url = \"https://news.ycombinator.com/item?id=42919502\"\n",
        "  print(f\"Scraping: {url}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "MYlZI621psoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ex6"
      ],
      "metadata": {
        "id": "n_V_p2erpu3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def main():\n",
        "  url = \"https://news.ycombinator.com/item?id=42919502\"\n",
        "  response = requests.get(url)\n",
        "  print(f\"Scraping: {url}\")\n",
        "  print(response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "H3nuWg_jpv06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ex7"
      ],
      "metadata": {
        "id": "ZHiwIZ9jp2Fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scraping: https://news.ycombinator.com/item?id=42919502\n",
        "# <Response [200]>\n",
        "# This is a comment and not code, so no syntax errors to fix.\n",
        "# If you'd like to scrape this URL using Python, you can try something like this:\n",
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "#\n",
        "# url = \"https://news.ycombinator.com/item?id=42919502\"\n",
        "# response = requests.get(url)\n",
        "#\n",
        "# if response.status_code == 200:\n",
        "#     soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "#     # Now you can extract data using soup.find(), soup.find_all(), etc.\n",
        "# else:\n",
        "#     print(f\"Error fetching URL. Status code: {response.status_code}\")"
      ],
      "metadata": {
        "id": "HPR44IuFp5bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ex8"
      ],
      "metadata": {
        "id": "TaGXl_TGp9vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def main():\n",
        "  url = \"https://news.ycombinator.com/item?id=42919502\"\n",
        "  response = requests.get(url)\n",
        "  print(f\"Scraping: {url}\")\n",
        "  print(response)\n",
        "  print(response.content) # Moved the print statement inside the main function\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "xYeLYloFqFYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyser la réponse HTTP avec « Beautifulsoup »"
      ],
      "metadata": {
        "id": "t64cMOPlqXcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ex1"
      ],
      "metadata": {
        "id": "mwgz9XNYqeCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "Hm6GreAGqZCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ex2"
      ],
      "metadata": {
        "id": "ZM5PvNtSqfT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def main():\n",
        "  url = \"https://news.ycombinator.com/item?id=42919502\"\n",
        "  response = requests.get(url)\n",
        "\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "  # find all elements with class=\"comment\"\n",
        "  elements = soup.find_all(class_=\"comment\")\n",
        "\n",
        "  # Show the number of elementd found\n",
        "  print(f\"Elements: {len(elements)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "8tbqw0HIqgKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extraire les commentaires individuels\n",
        "ex1"
      ],
      "metadata": {
        "id": "lwJpgivhShHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def main():\n",
        "  url = \"https://news.ycombinator.com/item?id=42919502\"\n",
        "  response = requests.get(url)\n",
        "\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "  # find all elements with class=\"ind\" and indent level = 0\n",
        "  elements = soup.find_all(class_=\"ind\" , indent=0)\n",
        "  # for each of this elements, find the next element\n",
        "  comments = [e.find_next(class_=\"comment\") for e in elements]\n",
        "\n",
        "  # Show the number of comments found\n",
        "  print(f\"Comments: {len(comments)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "eyrjgwpRSrvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ex2"
      ],
      "metadata": {
        "id": "C_iX0qIdSyP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def main():\n",
        "  url = \"https://news.ycombinator.com/item?id=42919502\"\n",
        "  response = requests.get(url)\n",
        "\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "  # find all elements with class=\"ind\" and indent level = 0\n",
        "  elements = soup.find_all(class_=\"ind\" , indent=0)\n",
        "  # for each of this elements, find the next element\n",
        "  comments = [e.find_next(class_=\"comment\") for e in elements]\n",
        "\n",
        "  # Show the number of comments found\n",
        "  print(f\"Comments: {len(comments)}\")\n",
        "\n",
        "  #Return the comments for use outside the function\n",
        "  return comments\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  comments = main() # Assign the result of main() to comments\n",
        "\n",
        "print(f\"Comments: {len(comments)}\") #Now, comments is defined and accessible here."
      ],
      "metadata": {
        "id": "uDLIOPMLS0Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ex3"
      ],
      "metadata": {
        "id": "GP5DPYa6TBIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show each comment (job post)\n",
        "for comment in comments:\n",
        "  print(comment)"
      ],
      "metadata": {
        "id": "lUxiRSPyTCaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nettoyer le texte de réponse\n",
        "# ex1"
      ],
      "metadata": {
        "id": "bzS5gIxJTE5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def main():\n",
        "  url = \"https://news.ycombinator.com/item?id=42919502\"\n",
        "  response = requests.get(url)\n",
        "\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "  # find all elements with class=\"ind\" and indent level = 0\n",
        "  elements = soup.find_all(class_=\"ind\" , indent=0)\n",
        "  # for each of this elements, find the next element\n",
        "  comments = [e.find_next(class_=\"comment\") for e in elements]\n",
        "\n",
        "  # show each comment (job post)\n",
        "  for comment in comments:\n",
        "    comment_text = comment.get_text()\n",
        "    print(comment_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "PBwl05gnTJ6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traitez le contenu récupéré pour obtenir des données utiles"
      ],
      "metadata": {
        "id": "6KfkXavTTRbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def main():\n",
        "  url = \"https://news.ycombinator.com/item?id=42919502\"\n",
        "  response = requests.get(url)\n",
        "\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "  # find all elements with class=\"ind\" and indent level = 0\n",
        "  elements = soup.find_all(class_=\"ind\" , indent=0)\n",
        "  # for each of this elements, find the next element\n",
        "  comments = [e.find_next(class_=\"comment\") for e in elements]\n",
        "\n",
        "  # Map of technologies keyword to search for\n",
        "  # and the occurence initialized at 0\n",
        "  keywords = {\"python\": 0, \"javascript\": 0, \"typescript\": 0, \"go\": 0, \"c#\": 0, \"java\": 0, \"rust\": 0 }\n",
        "\n",
        "  # show each comment (job post)\n",
        "  for comment in comments:\n",
        "    # get the comment text and lower case it\n",
        "    comment_text = comment.get_text().lower()\n",
        "\n",
        "    # split comment by space which create an array of words\n",
        "    words = comment_text.split(\" \")\n",
        "\n",
        "    print(words)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "VOEmtY09TUzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traitez le contenu récupéré pour obtenir des données utiles\n",
        "# ex1"
      ],
      "metadata": {
        "id": "Z0qNvjLVTY7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def main():\n",
        "  url = \"https://news.ycombinator.com/item?id=42919502\"\n",
        "  response = requests.get(url)\n",
        "\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "  # find all elements with class=\"ind\" and indent level = 0\n",
        "  elements = soup.find_all(class_=\"ind\" , indent=0)\n",
        "  # for each of this elements, find the next element\n",
        "  comments = [e.find_next(class_=\"comment\") for e in elements]\n",
        "\n",
        "  # Map of technologies keyword to search for\n",
        "  # and the occurence initialized at 0\n",
        "  keywords = {\"python\": 0, \"javascript\": 0, \"typescript\": 0, \"go\": 0, \"c#\": 0, \"java\": 0, \"rust\": 0 }\n",
        "\n",
        "  # show each comment (job post)\n",
        "  for comment in comments:\n",
        "    # get the comment text and lower case it\n",
        "    comment_text = comment.get_text().lower()\n",
        "\n",
        "    # split comment by space which create an array of words\n",
        "    words = comment_text.split(\" \")\n",
        "\n",
        "    print(words)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "cUbgqUxrTeiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex2"
      ],
      "metadata": {
        "id": "vDZmLyvdTjHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def main():\n",
        "  url = \"https://news.ycombinator.com/item?id=42919502\"\n",
        "  response = requests.get(url)\n",
        "\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "  # find all elements with class=\"ind\" and indent level = 0\n",
        "  elements = soup.find_all(class_=\"ind\" , indent=0)\n",
        "  # for each of this elements, find the next element\n",
        "  comments = [e.find_next(class_=\"comment\") for e in elements]\n",
        "\n",
        "  # Map of technologies keyword to search for\n",
        "  # and the occurence initialized at 0\n",
        "  keywords = {\"python\": 0, \"javascript\": 0, \"typescript\": 0, \"go\": 0, \"c#\": 0, \"java\": 0, \"rust\": 0 }\n",
        "\n",
        "  # show each comment (job post)\n",
        "  for comment in comments:\n",
        "    # get the comment text and lower case it\n",
        "    comment_text = comment.get_text().lower()\n",
        "\n",
        "    # split comment by space which create an array of words\n",
        "    words = comment_text.split(\" \")\n",
        "\n",
        "    # Use the string strip function inside the loop to process each comment's words\n",
        "    # and place all the characters we want to strip away\n",
        "    words = [w.strip(\".,/:;!@\") for w in words]\n",
        "\n",
        "    print(words)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "He9DJku0Toty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ex3"
      ],
      "metadata": {
        "id": "CMVLhPyTTz6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def main():\n",
        "  url = \"https://news.ycombinator.com/item?id=42919502\"\n",
        "  response = requests.get(url)\n",
        "\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "  # find all elements with class=\"ind\" and indent level = 0\n",
        "  elements = soup.find_all(class_=\"ind\" , indent=0)\n",
        "  # for each of this elements, find the next element\n",
        "  comments = [e.find_next(class_=\"comment\") for e in elements]\n",
        "\n",
        "  # Map of technologies keyword to search for\n",
        "  # and the occurence initialized at 0\n",
        "  keywords = {\"python\": 0, \"javascript\": 0, \"typescript\": 0, \"go\": 0, \"c#\": 0, \"java\": 0, \"rust\": 0 }\n",
        "\n",
        "  all_words = [] # Initialize an empty list to store all words\n",
        "\n",
        "  # show each comment (job post)\n",
        "  for comment in comments:\n",
        "    # get the comment text and lower case it\n",
        "    comment_text = comment.get_text().lower()\n",
        "\n",
        "    # split comment by space which create an array of words\n",
        "    words = comment_text.split(\" \")\n",
        "\n",
        "    # Use the string strip function inside the loop to process each comment's words\n",
        "    # and place all the characters we want to strip away\n",
        "    words = [w.strip(\".,/:;!@\") for w in words]\n",
        "\n",
        "    all_words.extend(words) # Add the words from this comment to the list\n",
        "\n",
        "    print(words)\n",
        "\n",
        "  # Use the string strip function to clean up the words and create a set of unique words\n",
        "  # and place all the characters we want to strip away\n",
        "  # Use a set to have unique words\n",
        "  unique_words = {w.strip(\".,/:;!@\") for w in all_words} # Use all_words here\n",
        "  print(unique_words) # Print the unique words\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "em9SBeePT30F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex4"
      ],
      "metadata": {
        "id": "PsNMo1SZUEjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "RWMH5ENOUFnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex5"
      ],
      "metadata": {
        "id": "gx6VUA_hUPa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def main():\n",
        "  url = \"https://news.ycombinator.com/item?id=42919502\"\n",
        "  response = requests.get(url)\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def main():\n",
        "  url = \"https://news.ycombinator.com/item?id=42919502\"\n",
        "  response = requests.get(url)\n",
        "\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "  # find all elements with class=\"ind\" and indent level = 0\n",
        "  elements = soup.find_all(class_=\"ind\")"
      ],
      "metadata": {
        "id": "GPhyB-f0UQIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualiser les données avec « matplotlib »"
      ],
      "metadata": {
        "id": "jAmGAQ3zUayn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install matplotlib"
      ],
      "metadata": {
        "id": "rHtaVl_DUeaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex2"
      ],
      "metadata": {
        "id": "XGlN24aGUhy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "DmQ8mIkoUj3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex3"
      ],
      "metadata": {
        "id": "vt20qaYJUm0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the keywords dictionary here if not defined globally\n",
        "keywords = {\"python\": 5, \"javascript\": 3, \"typescript\": 2, \"go\": 1, \"c#\": 4, \"java\": 2, \"rust\": 3}\n",
        "\n",
        "# plot a bar graph\n",
        "plt.bar(keywords.keys(), keywords.values())\n",
        "# Add labels\n",
        "plt.xlabel(\"Language\")\n",
        "plt.ylabel(\"# of Mentions\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ip8HawRJUn1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex4"
      ],
      "metadata": {
        "id": "6bs81i_iUv_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "5u55XlvjUw88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Et ensuite\n",
        "ex1"
      ],
      "metadata": {
        "id": "oGrf_MP1U0Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Installer les dépendances ---\n",
        "!pip install fastapi nest-asyncio pyngrok uvicorn selenium chromedriver-autoinstaller\n",
        "\n",
        "# --- Importations ---\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from fastapi import FastAPI\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import chromedriver_autoinstaller\n",
        "import csv\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# --- CONFIGURER TON AUTHTOKEN NGROK ---\n",
        "# Récupère ton token ici : https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "conf.get_default().auth_token = \"TON_AUTHTOKEN_ICI\"  # 🔥 Remplacer par ton vrai token !\n",
        "\n",
        "# --- Initialiser FastAPI ---\n",
        "app = FastAPI()\n",
        "\n",
        "# --- Fonction pour lancer Selenium (headless sur Colab) ---\n",
        "def get_driver():\n",
        "    chromedriver_autoinstaller.install()\n",
        "    options = Options()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    return driver\n",
        "\n",
        "# --- Scraping simple (titres de Google pour un mot-clé) ---\n",
        "def scrape_google(query):\n",
        "    driver = get_driver()\n",
        "    driver.get(f\"https://www.google.com/search?q={query}\")\n",
        "    titles = driver.find_elements(\"css selector\", \"h3\")\n",
        "    results = [{\"title\": t.text} for t in titles if t.text.strip()]\n",
        "    driver.quit()\n",
        "    return results\n",
        "\n",
        "# --- Sauvegarder résultats dans CSV ---\n",
        "def save_to_csv(data, filename=\"results.csv\"):\n",
        "    if not data:\n",
        "        return\n",
        "    keys = data[0].keys()\n",
        "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=keys)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "# --- Définir l'API FastAPI ---\n",
        "@app.post(\"/scrape/\")\n",
        "async def scrape(query: str):\n",
        "    data = scrape_google(query)\n",
        "    save_to_csv(data)\n",
        "    return {\"results\": data}\n",
        "\n",
        "# --- Lancer le serveur avec Ngrok ---\n",
        "# Connecter le port 8000 via Ngrok\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(f\"🚀 Ton API publique est ici : {ngrok_tunnel.public_url}/docs\")\n",
        "\n",
        "# --- Autoriser plusieurs boucles asyncio (nécessaire pour Colab) ---\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- Lancer FastAPI ---\n",
        "uvicorn.run(app, port=8000)\n"
      ],
      "metadata": {
        "id": "HhOyMjQ_U5Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mini projet à faire en binôme"
      ],
      "metadata": {
        "id": "SY5Rql_qvHXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 pandas\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# -------- Configuration --------\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# Liste de sites à scraper (exemples)\n",
        "SITES = {\n",
        "    \"indeed\": \"https://www.indeed.com/jobs?q={query}&l={location}\",\n",
        "    \"ziprecruiter\": \"https://www.ziprecruiter.com/candidate/search?search={query}&location={location}\",\n",
        "    \"monster\": \"https://www.monster.com/jobs/search/?q={query}&where={location}\"\n",
        "}\n",
        "\n",
        "# -------- Fonctions --------\n",
        "\n",
        "def get_indeed_jobs(query, location):\n",
        "    url = SITES[\"indeed\"].format(query=query, location=location)\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    jobs = []\n",
        "\n",
        "    for card in soup.select('a.tapItem'):\n",
        "        title = card.find('h2', {'class': 'jobTitle'}).text.strip() if card.find('h2', {'class': 'jobTitle'}) else ''\n",
        "        company = card.find('span', {'class': 'companyName'}).text.strip() if card.find('span', {'class': 'companyName'}) else ''\n",
        "        location = card.find('div', {'class': 'companyLocation'}).text.strip() if card.find('div', {'class': 'companyLocation'}) else ''\n",
        "        jobs.append({'Site': 'Indeed', 'Titre': title, 'Entreprise': company, 'Lieu': location})\n",
        "    return jobs\n",
        "\n",
        "def get_ziprecruiter_jobs(query, location):\n",
        "    url = SITES[\"ziprecruiter\"].format(query=query, location=location)\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    jobs = []\n",
        "\n",
        "    for card in soup.select('article.job_result'):\n",
        "        title = card.find('a', {'class': 'job_link'}).text.strip() if card.find('a', {'class': 'job_link'}) else ''\n",
        "        company = card.find('a', {'class': 't_org_link'}).text.strip() if card.find('a', {'class': 't_org_link'}) else ''\n",
        "        location_tag = card.find('ul', {'class': 'job_location'})\n",
        "        location = location_tag.text.strip() if location_tag else ''\n",
        "        jobs.append({'Site': 'ZipRecruiter', 'Titre': title, 'Entreprise': company, 'Lieu': location})\n",
        "    return jobs\n",
        "\n",
        "def get_monster_jobs(query, location):\n",
        "    url = SITES[\"monster\"].format(query=query, location=location)\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    jobs = []\n",
        "\n",
        "    for card in soup.select('section.card-content'):\n",
        "        title_tag = card.find('h2', {'class': 'title'})\n",
        "        company_tag = card.find('div', {'class': 'company'})\n",
        "        location_tag = card.find('div', {'class': 'location'})\n",
        "\n",
        "        title = title_tag.text.strip() if title_tag else ''\n",
        "        company = company_tag.text.strip() if company_tag else ''\n",
        "        location = location_tag.text.strip() if location_tag else ''\n",
        "        jobs.append({'Site': 'Monster', 'Titre': title, 'Entreprise': company, 'Lieu': location})\n",
        "    return jobs\n",
        "\n",
        "def filter_jobs(jobs, entreprise=None, secteur=None, location=None):\n",
        "    df = pd.DataFrame(jobs)\n",
        "    if entreprise:\n",
        "        df = df[df['Entreprise'].str.contains(entreprise, case=False, na=False)]\n",
        "    if location:\n",
        "        df = df[df['Lieu'].str.contains(location, case=False, na=False)]\n",
        "    if secteur:\n",
        "        df = df[df['Titre'].str.contains(secteur, case=False, na=False)]\n",
        "    return df\n",
        "\n",
        "# -------- Main --------\n",
        "\n",
        "def main():\n",
        "    query = input(\"Mot-clé du poste recherché : \")\n",
        "    location = input(\"Ville (ex: Paris, France ou New York, USA) : \")\n",
        "\n",
        "    print(\"\\nScraping en cours... Patientez quelques secondes.\")\n",
        "    all_jobs = []\n",
        "\n",
        "    # Chaque site est scrappé séparément\n",
        "    all_jobs.extend(get_indeed_jobs(query, location))\n",
        "    time.sleep(2)  # Pause pour éviter le blocage\n",
        "    all_jobs.extend(get_ziprecruiter_jobs(query, location))\n",
        "    time.sleep(2)\n",
        "    all_jobs.extend(get_monster_jobs(query, location))\n",
        "\n",
        "    print(f\"Total d'offres récupérées : {len(all_jobs)}\")\n",
        "\n",
        "    # Application de filtres\n",
        "    filter_entreprise = input(\"\\nFiltrer par nom d'entreprise (laisser vide pour ignorer) : \")\n",
        "    filter_secteur = input(\"Filtrer par mot-clé de secteur (laisser vide pour ignorer) : \")\n",
        "    filter_location = input(\"Filtrer par ville spécifique (laisser vide pour ignorer) : \")\n",
        "\n",
        "    df_filtered = filter_jobs(all_jobs, entreprise=filter_entreprise, secteur=filter_secteur, location=filter_location)\n",
        "    print(\"\\nOffres filtrées :\")\n",
        "    print(df_filtered)\n",
        "\n",
        "    # Sauvegarde CSV\n",
        "    df_filtered.to_csv('offres_emploi_scrapees.csv', index=False, encoding='utf-8')\n",
        "    print(\"\\n✅ Offres sauvegardées dans 'offres_emploi_scrapees.csv'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# -------- Configuration --------\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# Liste de sites à scraper (exemples)\n",
        "SITES = {\n",
        "    \"indeed\": \"https://www.indeed.com/jobs?q={query}&l={location}\",\n",
        "    \"ziprecruiter\": \"https://www.ziprecruiter.com/candidate/search?search={query}&location={location}\",\n",
        "    \"monster\": \"https://www.monster.com/jobs/search/?q={query}&where={location}\"\n",
        "}\n",
        "\n",
        "# -------- Fonctions --------\n",
        "\n",
        "def get_indeed_jobs(query, location):\n",
        "    url = SITES[\"indeed\"].format(query=query, location=location)\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    jobs = []\n",
        "\n",
        "    for card in soup.select('a.tapItem'):\n",
        "        title = card.find('h2', {'class': 'jobTitle'}).text.strip() if card.find('h2', {'class': 'jobTitle'}) else ''\n",
        "        company = card.find('span', {'class': 'companyName'}).text.strip() if card.find('span', {'class': 'companyName'}) else ''\n",
        "        location = card.find('div', {'class': 'companyLocation'}).text.strip() if card.find('div', {'class': 'companyLocation'}) else ''\n",
        "        jobs.append({'Site': 'Indeed', 'Titre': title, 'Entreprise': company, 'Lieu': location})\n",
        "    return jobs\n",
        "\n",
        "def get_ziprecruiter_jobs(query, location):\n",
        "    url = SITES[\"ziprecruiter\"].format(query=query, location=location)\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    jobs = []\n",
        "\n",
        "    for card in soup.select('article.job_result'):\n",
        "        title = card.find('a', {'class': 'job_link'}).text.strip() if card.find('a', {'class': 'job_link'}) else ''\n",
        "        company = card.find('a', {'class': 't_org_link'}).text.strip() if card.find('a', {'class': 't_org_link'}) else ''\n",
        "        location_tag = card.find('ul', {'class': 'job_location'})\n",
        "        location = location_tag.text.strip() if location_tag else ''\n",
        "        jobs.append({'Site': 'ZipRecruiter', 'Titre': title, 'Entreprise': company, 'Lieu': location})\n",
        "    return jobs\n",
        "\n",
        "def get_monster_jobs(query, location):\n",
        "    url = SITES[\"monster\"].format(query=query, location=location)\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    jobs = []\n",
        "\n",
        "    for card in soup.select('section.card-content'):\n",
        "        title_tag = card.find('h2', {'class': 'title'})\n",
        "        company_tag = card.find('div', {'class': 'company'})\n",
        "        location_tag = card.find('div', {'class': 'location'})\n",
        "\n",
        "        title = title_tag.text.strip() if title_tag else ''\n",
        "        company = company_tag.text.strip() if company_tag else ''\n",
        "        location = location_tag.text.strip() if location_tag else ''\n",
        "        jobs.append({'Site': 'Monster', 'Titre': title, 'Entreprise': company, 'Lieu': location})\n",
        "    return jobs\n",
        "\n",
        "def filter_jobs(jobs, entreprise=None, secteur=None, location=None):\n",
        "    df = pd.DataFrame(jobs)\n",
        "    if entreprise:\n",
        "        df = df[df['Entreprise'].str.contains(entreprise, case=False, na=False)]\n",
        "    if location:\n",
        "        df = df[df['Lieu'].str.contains(location, case=False, na=False)]\n",
        "    if secteur:\n",
        "        df = df[df['Titre'].str.contains(secteur, case=False, na=False)]\n",
        "    return df\n",
        "\n",
        "# -------- Main --------\n",
        "\n",
        "def main():\n",
        "    query = input(\"Mot-clé du poste recherché : \")\n",
        "    location = input(\"Ville (ex: Paris, France ou New York, USA) : \")\n",
        "\n",
        "    print(\"\\nScraping en cours... Patientez quelques secondes.\")\n",
        "    all_jobs = []\n",
        "\n",
        "    # Chaque site est scrappé séparément\n",
        "    all_jobs.extend(get_indeed_jobs(query, location))\n",
        "    time.sleep(2)  # Pause pour éviter le blocage\n",
        "    all_jobs.extend(get_ziprecruiter_jobs(query, location))\n",
        "    time.sleep(2)\n",
        "    all_jobs.extend(get_monster_jobs(query, location))\n",
        "\n",
        "    print(f\"Total d'offres récupérées : {len(all_jobs)}\")\n",
        "\n",
        "    # Application de filtres\n",
        "    filter_entreprise = input(\"\\nFiltrer par nom d'entreprise (laisser vide pour ignorer) : \")\n",
        "    filter_secteur = input(\"Filtrer par mot-clé de secteur (laisser vide pour ignorer) : \")\n",
        "    filter_location = input(\"Filtrer par ville spécifique (laisser vide pour ignorer) : \")\n",
        "\n",
        "    df_filtered = filter_jobs(all_jobs, entreprise=filter_entreprise, secteur=filter_secteur, location=filter_location)\n",
        "    print(\"\\nOffres filtrées :\")\n",
        "    print(df_filtered)\n",
        "\n",
        "    # Sauvegarde CSV\n",
        "    df_filtered.to_csv('offres_emploi_scrapees.csv', index=False, encoding='utf-8')\n",
        "    print(\"\\n✅ Offres sauvegardées dans 'offres_emploi_scrapees.csv'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "EM-SXOpKvKgc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}